<!DOCTYPE html>

<html lang="en">



<head>

    <meta charset="UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Hel's Domain</title>

    <script src="https://kjur.github.io/jsrsasign/jsrsasign-latest-all-min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jsencrypt/3.1.0/jsencrypt.min.js"></script>

    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #f9f9f9;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }

        #main {
            display: none;
        }

        #password-form {
            margin: 0 auto;
            width: 300px;
            padding: 20px;
            background-color: #fff;
            border-radius: 5px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        #password-input {
            width: 100%;
            margin-bottom: 10px;
            padding: 8px;
            font-size: 16px;
            border: 1px solid #ccc;
            border-radius: 4px;
            box-sizing: border-box;
        }

        #submit-btn {
            width: 100%;
            padding: 10px;
            background-color: #007bff;
            color: #fff;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }

        .container {
            display: flex;
            gap: 20px;
            flex-direction: column;
            align-content: center;
            flex-wrap: wrap;
        }

        .button {
            font-size: 16px;
            padding: 12px 24px;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.3s ease;
            outline: none;
        }

        .button:hover {
            transform: translateY(-3px);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .button-icon {
            display: flex;
            flex-direction: column;
            align-items: center;
            color: #fff;
            text-decoration: none;
        }

        .button-icon i {
            margin-bottom: 6px;
            font-size: 24px;
        }

        .button-primary {
            background-color: #4CAF50;
            color: #fff;
        }

        .button-secondary {
            background-color: #3498db;
            color: #fff;
        }

        .button-warning {
            background-color: #f39c12;
            color: #fff;
        }

        .button-info {
            background-color: #2ecc71;
            color: #fff;
        }
    </style>
</head>

<body>
    <div id="password-form">
        <h2>Please enter the password</h2>
        <input type="password" id="password-input" placeholder="Password">
        <button id="submit-btn">Submit</button>
    </div>

    <div id="main">

        <div class="container">
                <a href="https://zoro1801.github.io/dante/Encryptor.html"></a><span>Encrypter</span>
            <button class="button button-icon button-primary" onclick="copyText(text1)">
                <i class="fas fa-chart-line"></i>
                <span>P1(A) KNN</span>
            </button>
            <button class="button button-icon button-secondary" onclick="copyText(text2)">
                <i class="fas fa-chart-bar"></i>
                <span>P1(B) KMeans</span>
            </button>

            <button class="button button-icon button-warning" onclick="copyText(text3)">
                <i class="fas fa-chart-pie"></i>
                <span>P2 Naive Bayes(Gaussian)</span>
            </button>

            <button class="button button-icon button-info" onclick="copyText(text4)">
                <i class="fas fa-chart-area"></i>
                <span>P3 OverFitting</span>
            </button>

            <button class="button button-icon button-primary" onclick="copyText(text5)">
                <i class="fas fa-line-chart"></i>
                <span>P4 Linear Regression </span>
                <img src="https://raw.githubusercontent.com/zoro1801/dante/main/Screenshot%202024-04-12%20210931.png"
                    alt="">
            </button>

            <button class="button button-icon button-secondary" onclick="copyText(text6)">
                <i class="fas fa-bar-chart"></i>
                <span>P5 SVC/SVM</span>
            </button>

            <button class="button button-icon button-warning" onclick="copyText(text7)">
                <i class="fas fa-pie-chart"></i>
                <span>P6(A) Kmeans Random Sample Data</span>
            </button>

            <button class="button button-icon button-info" onclick="copyText(text8)">
                <i class="fas fa-chart-line"></i>
                <span>P6(B) Kmeans Income & Spending Data</span>
            </button>

            <button class="button button-icon button-primary" onclick="copyText(text9)">
                <i class="fas fa-chart-bar"></i>
                <span>P7 Decision Tree</span>
            </button>

            <button class="button button-icon button-secondary" onclick="copyText(text10)">
                <i class="fas fa-chart-pie"></i>
                <span>P8 Agglomerative Clustering/Hierarchical Clustering</span>
            </button>

            <button class="button button-icon button-warning" onclick="copyText(text11)">
                <i class="fas fa-chart-area"></i>
                <span>Assignment 1 Confusion Matrix Multi Class Classification</span>
            </button>

            <button class="button button-icon button-info" onclick="copyText(text12)">
                <i class="fas fa-line-chart"></i>
                <span>Assignment 2 Multi Class Classification Using KNN</span>
            </button>

            <button class="button button-icon button-primary" onclick="copyText(text13)">
                <i class="fas fa-bar-chart"></i>
                <span>Assignment 3 Multi Label Classification</span>
            </button>

            <button class="button button-icon button-secondary" onclick="copyText(text14)">
                <i class="fas fa-pie-chart"></i>
                <span>Assignment 4 Income Happiness Linear Regression</span>
            </button>
        </div>
        <p id="copiedMsg" style="display: block;"></p>
    </div>
    <script>

        var text1 = `
#import Necessary libraries
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import  accuracy_score
import matplotlib.pyplot as plt

#Load the data

iris = load_iris()

iris.data.shape

#Split the dataset into features and target

X,Y = iris.data,iris.target

#Split the dataset into training and testing set

X_Train,X_Test,Y_Train,Y_Test = train_test_split(X,Y,test_size= 0.33,random_state=99)

#initialize a KNearest Classifier

knn_cls = KNeighborsClassifier(n_neighbors = 4)

#Train the model on the training data

knn_cls.fit(X_Train,Y_Train)

#Make prediction on test data

pred = knn_cls.predict(X_Test)

#Evaluate the model accuracy

acc = accuracy_score(Y_Test,pred)
print(f"Accuracy :{acc:.2f")

# Plotting the scatter graph
plt.scatter(X_Test[:, 0], X_Test[:, 1], c=Y_Test, cmap='viridis')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

The k-Nearest Neighbors (KNN) algorithm is a simple and effective classification algorithm used in supervised learning. It's a non-parametric and lazy learning algorithm, meaning it doesn't make any assumptions about the underlying data distribution and it doesn't learn a model during training. Instead, it memorizes the entire training dataset and makes predictions based on the similarity of new data points to the training data.


`;

        var text2 = `
# Import necessary libraries
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()

# Split the dataset into features and target
X, Y = iris.data, iris.target

# Initialize KMeans clustering with 3 clusters
Km = KMeans(n_clusters=3, random_state=40)

# Fit KMeans to the data
Km.fit(X)

# Get cluster labels
cluster_labels = Km.labels_

# Plot the clusters
plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')
plt.title("Scatter Plot of Iris Dataset using K-Means Clustering\n")
plt.show()


KMeans is a popular unsupervised machine learning algorithm used for clustering data into groups. The goal of KMeans is to partition a dataset into K distinct, non-overlapping subsets (clusters) where each data point belongs to the cluster with the nearest mean (centroid).

The algorithm iteratively assigns each data point to the nearest centroid and then recalculates the centroids based on the mean of the points assigned to each cluster. This process continues until the centroids no longer change significantly or a specified number of iterations is reached.

KMeans is commonly used in various applications such as customer segmentation, image compression, and anomaly detection. It is relatively fast and scalable, making it suitable for large datasets. However, it requires specifying the number of clusters (K) in advance, and its performance can be sensitive to the initialization of centroids.
`;

        var text3 = `

        from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Load the Iris dataset
df = load_iris()

# Split the dataset into features (x) and target (y)
x = df.data
y = df.target

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

# Initialize and train the Gaussian Naive Bayes classifier
gnb = GaussianNB()
gnb.fit(x_train, y_train)

# Example user input
user_input = [[5, 3, 5, 8]]

# Predict the species with the user input
predicted_index = gnb.predict(user_input)
predicted_species = df.target_names[predicted_index][0]

# Make predictions on the test set
pred = gnb.predict(x_test)

# Calculate accuracy score, confusion matrix, and classification report
accuracy = accuracy_score(y_test, pred)
conf_matrix = confusion_matrix(y_test, pred)
class_report = classification_report(y_test, pred)

# Print the results
print(f'User Input Predicted Species: {predicted_species}')
print(f'Accuracy Score: {accuracy:.2f}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')


Naive Bayes is a simple yet powerful algorithm used for classification tasks in machine learning. It's based on Bayes' theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Naive Bayes assumes that the features in the dataset are independent of each other, which is a naive assumption but often holds true in practice.

Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It's suitable for continuous data, where each feature is assumed to be normally distributed. In Gaussian Naive Bayes, the likelihood of the features is assumed to be Gaussian, and the parameters of the Gaussian distribution (mean and variance) are estimated from the training data.

In summary, Gaussian Naive Bayes is specifically designed for datasets with continuous features, assuming that each class is associated with a Gaussian distribution of feature values. It's widely used in classification tasks, especially when dealing with real-valued features.
`;

        var text4 = `

import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
X = np.linspace(0, 5, 20)  # 20 evenly spaced points between 0 and 5
y = 2 * X + 1 + np.random.randn(20)  # Linear relationship with noise

# Fit polynomial regression models of different degrees
degrees = [1, 5, 10]  # degrees of the polynomial models
colors = ['blue', 'red', 'green']  # colors for plotting

plt.scatter(X, y, color='black', label='Data Points')

for degree, color in zip(degrees, colors):
    # Fit polynomial
    coef = np.polyfit(X, y, degree)
    poly = np.poly1d(coef)

    # Generate points for plotting
    x_values = np.linspace(0, 5, 100)
    y_values = poly(x_values)

    # Plotting
    plt.plot(x_values, y_values, label=f'Degree {degree}', color=color)

plt.xlabel('X')
plt.ylabel('y')
plt.title('Polynomial Regression Example')
plt.legend()
plt.grid(True)
plt.show()


Overfitting occurs when a machine learning model learns the training data too well, capturing noise or random fluctuations in the data rather than the underlying patterns or relationships. This leads to poor generalization performance on unseen data, as the model becomes too specific to the training set.
`;

        var text5 = `

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv("Salary_Data.csv")

# Extract features (Years of Experience) and target (Salary)
x = data['YearsExperience'].values.reshape(-1, 1)
y = data["Salary"].values

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=99)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(x_train, y_train)

# Make predictions on the testing set
predictions = model.predict(x_test)

# Calculate R-squared score
r2 = r2_score(y_test, predictions)
print("R-squared Score:", r2)
print("Dhanraj Chinta - KFMSCIT007")

# Plot the actual vs. predicted values
plt.scatter(x_test, y_test, color='blue', label='Actual')
plt.scatter(x_test, predictions, color='red', label='Predicted')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.title('Linear Regression: Salary Prediction\n')
plt.legend()
plt.show()


Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the independent variables (features) and the dependent variable (target). The goal of linear regression is to find the best-fitting straight line that describes the relationship between the variables. This line is represented by the equation:

\[ y = mx + b \]

where:
- \( y \) is the dependent variable (target),
- \( x \) is the independent variable (feature),
- \( m \) is the slope of the line, and
- \( b \) is the y-intercept.

Mean squared error (MSE) is a common metric used to evaluate the performance of regression models, including linear regression. It measures the average squared difference between the actual values (observed) and the predicted values (estimated) by the model. The formula for MSE is:



where:
- \( n \) is the number of observations,
- \( y_i \) is the actual value of the dependent variable for observation \( i \),
- \( \hat{y}_i \) is the predicted value of the dependent variable for observation \( i \), and
- \( \sum \) denotes summation over all observations.

`;

        var text6 = `

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.svm import SVC

# Load the iris dataset
data = pd.read_csv('iris.csv')

# Separate features (X) and target (Y)
X = data.iloc[:, :-1].values
Y = data.iloc[:, -1].values

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=10)

# Initialize and train the Support Vector Classifier (SVC)
model = SVC()
model.fit(X_train, Y_train)

# Predict the target labels for the test set
Y_pred = model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred, average='macro')
recall = recall_score(Y_test, Y_pred, average='macro')
f1 = f1_score(Y_test, Y_pred, average='macro')

print("")
print("---------------------------")
# Print the evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)


Support Vector Machine (SVM):

SVM is a supervised learning algorithm used mainly for classification tasks but can also be used for regression tasks.
It works by finding the hyperplane that best separates different classes in the feature space. The goal is to maximize the margin between the hyperplane and the nearest data points (support vectors).
SVM can handle both linear and non-linear data by using different kernel functions such as linear, polynomial, radial basis function (RBF), etc.
Support Vector Classifier (SVC):

SVC is a specific implementation of SVM for classification tasks. It is essentially an SVM that is specifically used for classification.
In scikit-learn, the SVC class is used for implementing SVM-based classification models.
SVC aims to find the hyperplane that best separates different classes in the feature space.
`;

        var text7 = `
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Sample data
data = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.5], [7, 9], [9, 10], [5.5, 8.5]])

# Number of clusters (k)
k = 3

# Applying KMeans clustering
kmeans = KMeans(n_clusters=k, random_state=0)
clusters_labels = kmeans.fit_predict(data)

# Plotting clustered data
plt.scatter(data[:, 0], data[:, 1], c=clusters_labels,label='Data Points')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=140, c='green', label='Cluster Centers')
plt.xlabel('X Coordinate')
plt.ylabel('Y Coordinate')
plt.title('\nKMeans Clustering (k=' + str(k) + ')')
plt.legend()
plt.show()


K-Means is a popular unsupervised machine learning algorithm used for clustering data into groups or clusters based on similarities in the feature space. The algorithm aims to partition the data into a predefined number of clusters, where each data point belongs to the cluster with the nearest mean (centroid).
`;

        var text8 = `

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Sample data representing (Income and Spending)
data = pd.DataFrame({'Income': [25000, 50000, 75000, 100000, 125000, 150000, 175000, 200000],
                     'Spending': [15000, 25000, 35000, 45000, 55000, 65000, 75000, 85000]})

# Number of clusters (k)
k = 3

# Applying KMeans clustering
kmeans = KMeans(n_clusters=k, random_state=0)
clusters_labels = kmeans.fit_predict(data)

# Plotting clustered data
plt.scatter(data['Income'], data['Spending'], c=clusters_labels, cmap='viridis', label='Data Points')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=140, c='green', label='Cluster Centers')
plt.xlabel('Income')
plt.ylabel('Spending')
plt.title('Dhanraj Chinta - KFMSCIT007\nKMeans Clustering (k=' + str(k) + ')')
plt.legend()
plt.show()


K-Means is a popular unsupervised machine learning algorithm used for clustering data into groups or clusters based on similarities in the feature space. The algorithm aims to partition the data into a predefined number of clusters, where each data point belongs to the cluster with the nearest mean (centroid).
`;

        var text9 = `

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load the Iris dataset
data = pd.read_csv('iris.csv')
# Separate features (X) and target (y)
X = data.drop('species', axis=1)
y = data['species']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Decision Tree Classifier
model = DecisionTreeClassifier()
model.fit(X_train, y_train)
# Make predictions on the testing set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print classification report for more evaluation metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Visualize the decision tree
plt.figure(figsize=(12, 8))
plot_tree(model, filled=True, feature_names=X.columns, class_names=y.unique().astype(str))
plt.title("Decision Tree Visualization \n")
plt.show()

A decision tree is a popular supervised machine learning algorithm used for both classification and regression tasks. It's a tree-like structure where each internal node represents a "decision" based on a feature attribute, each branch represents the outcome of that decision, and each leaf node represents the final prediction or classification.

In a decision tree:

Root Node: The top node of the tree, representing the entire dataset. It contains the feature that best splits the data into distinct classes or categories.

Internal Nodes: These nodes represent a decision based on a feature attribute. They split the dataset into smaller subsets.

Branches: Branches emanating from each internal node represent the outcome of the decision. Each branch leads to a child node based on the feature's value.

Leaf Nodes: These are the terminal nodes of the tree. They represent the final outcome or prediction. For classification tasks, each leaf node corresponds to a class label. For regression tasks, leaf nodes contain continuous values.

    `;

        var text10 = `
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, completeness_score

# Sample dataset
data = np.array([[1, 1], [5, 5], [8, 8], [1, 0], [5, 4], [8, 1]])

# Parameters
n_clusters = 2
linkage = 'ward'  # Method for calculating linkage
# Perform agglomerative clustering
model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)
cluster_labels = model.fit_predict(data)

# Calculate silhouette score
silhouette = silhouette_score(data, cluster_labels)
print("Silhouette Score:", silhouette)

# Calculate completeness score if applicable
g = None  # Ground truth labels, if available
if linkage == 'ward' and g is not None:
    completeness = completeness_score(g, cluster_labels)
    print("Completeness Score:", completeness)
else:
    print("Completeness Score is not applicable for the selected linkage method.")



Agglomerative clustering is a hierarchical clustering technique used in unsupervised machine learning to group similar data points into clusters. In this approach, each data point initially represents a single cluster, and the algorithm progressively merges clusters based on their similarity until all data points belong to a single cluster or a stopping criterion is met.

The basic idea behind agglomerative clustering is to iteratively merge the two closest clusters into a single cluster until a desired number of clusters or a specified linkage criterion is achieved. The "linkage criterion" determines how the distance between clusters is measured and influences the merging process.
    `;
        var text11 = `
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

actual = np.array(['Apple', 'Orange', 'Banana', 'Apple', 'Orange', 'Apple', 'Orange', 'Banana', 'Banana', 'Apple'])
predicted = np.array(['Apple', 'Orange', 'Orange', 'Apple', 'Orange', 'Apple', 'Orange', 'Banana', 'Banana', 'Orange'])

cm = confusion_matrix(actual,predicted)

sns.heatmap(cm,annot=True,fmt='g',xticklabels=['Apple', 'Orange', 'Banana'],yticklabels=['Apple', 'Orange', 'Banana'])
plt.ylabel('Prediction', fontsize=13)
plt.xlabel('Actual', fontsize=13)
plt.title('Confusion Matrix', fontsize=17)
plt.show()

print(classification_report(actual, predicted))

A confusion matrix is a performance measurement tool used in classification problems to evaluate the accuracy of a machine learning model. It is a table that describes the performance of a classification model on a set of test data for which the true values are known.

In a confusion matrix:

Each row represents the actual class labels.
Each column represents the predicted class labels.
The four main components of a confusion matrix are:

True Positives (TP): The cases where the model correctly predicted the positive class.

True Negatives (TN): The cases where the model correctly predicted the negative class.

False Positives (FP): The cases where the model incorrectly predicted the positive class (it predicted positive when the actual class is negative).

False Negatives (FN): The cases where the model incorrectly predicted the negative class (it predicted negative when the actual class is positive).
    `;
        var text12 = `
# Import necessary libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Target variable (labels)
#Apply Standard Scaler on feature
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)

# Initialize the k-NN classifier (with k=3)
knn_classifier = KNeighborsClassifier(n_neighbors=3)

# Train the classifier on the training data
knn_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn_classifier.predict(X_test)

#Evaluate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

Multi-class classification is a type of supervised learning task in machine learning where the goal is to classify input data points into one of multiple predefined classes or categories. In multi-class classification:

Input Data: Each data point consists of a set of features or attributes. These features are used to make predictions about the class labels.

Output Classes: There are more than two possible classes or categories that the input data can belong to. For example, in a multi-class classification problem for handwritten digit recognition, the classes would be the digits 0 through 9.

Prediction: The model predicts the class label or category that best represents the input data point.

Evaluation: The performance of the model is typically evaluated using metrics such as accuracy, precision, recall, and F1-score.

Examples of multi-class classification tasks include handwritten digit recognition (predicting digits from 0 to 9), sentiment analysis with multiple sentiment categories, and classifying different types of animals.

    `;
        var text13 = `
    from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multioutput import MultiOutputClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

documents = [
    "This study investigates the impact of diet on health outcomes.",
    "New advancements in medical technology revolutionize healthcare.",
    "A review of recent breakthroughs in cancer treatment research.",
    "The role of genetics in determining susceptibility to diseases.",
    "Understanding the connection between mental health and physical well-being."
]

# Multi-label classification labels
labels = {
    'Healthcare': [1, 0],
    'Medical Research': [0, 1]
}

# Repeat documents and labels to create 100 samples
documents *= 20  # Repeat each document 20 times to get 100 documents

# Repeat labels accordingly
y = [labels['Healthcare'], labels['Healthcare'], labels['Medical Research'],labels ['Medical Research'], [1, 1]] * 20

# Print length to verify (Optional)
print("Number of documents:", len(documents))
print("Number of labels:", len(y))

# Splitting the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(documents, y, test_size=0.2, random_state=42)

# Vectorizing the text data
tfidf_vectorizer = TfidfVectorizer()nnnn
x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)
x_test_tfidf = tfidf_vectorizer.transform(x_test)

# Multi-output classifier with Naive Bayes
classifier = MultiOutputClassifier(MultinomialNB())

# Training the classifier
classifier.fit(x_train_tfidf, y_train)

# Predictions
predictions = classifier.predict(x_test_tfidf)



Multi-label classification is a supervised learning problem where each instance (or data point) can be assigned multiple labels or categories simultaneously. In contrast to multi-class classification, where each instance belongs to exactly one class, multi-label classification allows for more complex scenarios where instances may have multiple class labels associated with them.

Key characteristics of multi-label classification:

Multiple Labels: Each instance can be associated with one or more labels from a set of predefined labels.

Label Dependencies: There can be dependencies or correlations between different labels. For example, in a document classification task, a document may belong to multiple categories simultaneously, such as "sports", "politics", and "entertainment".

Output Representation: The output of a multi-label classification model is typically a binary vector where each element represents whether or not the corresponding label is present for a given instance.
    `;
        var text14 = `
lpsruw sdqgdv dv sg
iurp vnohduq.prgho_vhohfwlrq lpsruw wudlq_whvw_vsolw
iurp vnohduq.olqhdu_prgho lpsruw OlqhduUhjuhvvlrq
iurp vnohduq.phwulfv lpsruw phdq_vtxduhg_huuru,dffxudfb_vfruh

# Ordg wkh gdwd
gdwd = sg.uhdg_fvy('Irup Uhvsrqvhv.fvy')  # Uhsodfh 'uhvsrqvhv.fvy' zlwk wkh sdwk wr brxu FVY iloh
sulqw("Gdwd ordghg vxffhvvixoob.")
gdwd.khdg()

gdwd.gursqd(lqsodfh=Wuxh)

A = gdwd[['Vdodub']].ydoxhv
b = gdwd['Vdwlvidfwlrq'].ydoxhv

# Vsolw wkh gdwd lqwr wudlqlqj dqg whvwlqj vhwv
A_wudlq, A_whvw, b_wudlq, b_whvw = wudlq_whvw_vsolw(A, b, whvw_vlch=0.2, udqgrp_vwdwh=42)

# Wudlq wkh prgho
prgho = OlqhduUhjuhvvlrq()
prgho.ilw(A_wudlq, b_wudlq)

# Pdnh suhglfwlrqv
b_suhg = prgho.suhglfw(A_whvw)

# Hydoxdwh wkh prgho
pvh = phdq_vtxduhg_huuru(b_whvw, b_suhg)
sulqw("Phdq Vtxduhg Huuru:", pvh)

    
    `;

function caesarCipherEncrypt(text, shift) {
  let encryptedText = "";
  for (let i = 0; i < text.length; i++) {
    let char = text[i];
    if (char.match(/[a-z]/i)) {
      let code = text.charCodeAt(i);
      if (code >= 65 && code <= 90) {
        char = String.fromCharCode(((code - 65 + shift) % 26) + 65);
      } else if (code >= 97 && code <= 122) {
        char = String.fromCharCode(((code - 97 + shift) % 26) + 97);
      }
    }
    encryptedText += char;
  }
  return encryptedText;
}
var a = caesarCipherEncrypt(text14,3)
console.log(a)
function Unravel(encryptedText, shift) {
  return caesarCipherEncrypt(encryptedText, (26 - shift) % 26);
}
        function copyText(text) {
            var shift = 3;
            var decryptedText = Unravel(text, shift);
            navigator.clipboard.writeText(decryptedText).then(function () {

                document.getElementById('copiedMsg').innerHTML = "Text Copied"

            }).catch(function (err) {

                console.error('Unable to copy text', err);

            });

        }
        const passwordInput = document.getElementById('password-input');
        const submitBtn = document.getElementById('submit-btn');

        submitBtn.addEventListener('click', function () {
            const password = passwordInput.value.trim();
                var hashpwd = caesarCipherEncrypt(password,3)
                if (hashpwd === 'qdklghqd') {
                    document.getElementById('main').style.display = 'block';
                    document.getElementById('password-form').style.display = 'none';
                } else {
                    alert('Incorrect password. Please try again.');
                    passwordInput.value = '';
                }
        });
    </script>



</body>



</html>
