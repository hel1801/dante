<!DOCTYPE html>

<html lang="en">



<head>

    <meta charset="UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Copy Text Buttons</title>

    <style>

        button {

            margin: 5px;

        }



        body {

            background-color: #f0f0f0;

            /* Light gray background */

            font-family: Arial, sans-serif;

            text-align: center;

        }



        button {

            margin: 10px;

            padding: 10px;

            font-size: 16px;

            background-color: #ddd;

            /* Light gray button background */

            border: 1px solid #aaa;

            /* Dark gray border */

            cursor: pointer;

        }



        button:hover {

            background-color: #ccc;

            /* Slightly darker background on hover */

        }

    </style>

</head>



<body>



    <button onclick="copyText(text1)"">P1(A) KNN</button>

  <button onclick=" copyText(text2)">P1(B) KMeans</button>

    <button onclick="copyText(text3)">P2 Naive Bayes(Gaussian)</button>

    <button onclick="copyText(text4)">P3 OverFitting</button>

    <button onclick="copyText(text5)">P4 Linear Regression</button>

    <button onclick="copyText(text6)">P5 SVC/SVM</button>

    <button onclick="copyText(text7)">P6(A) Kmeans Random Sample Data</button>

    <button onclick="copyText(text8)">P6(B) Kmeans Income & Spending Data</button>

    <button onclick="copyText(text9)">P7 Decision Tree</button>

    <p id="copiedMsg"></p>

    <script>

        var text1 = `#import Necessary libraries
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import  accuracy_score
import matplotlib.pyplot as plt

#Load the data

iris = load_iris()

iris.data.shape

#Split the dataset into features and target

X,Y = iris.data,iris.target

#Split the dataset into training and testing set

X_Train,X_Test,Y_Train,Y_Test = train_test_split(X,Y,test_size= 0.33,random_state=99)

#initialize a KNearest Classifier

knn_cls = KNeighborsClassifier(n_neighbors = 4)

#Train the model on the training data

knn_cls.fit(X_Train,Y_Train)

#Make prediction on test data

pred = knn_cls.predict(X_Test)

#Evaluate the model accuracy

acc = accuracy_score(Y_Test,pred)
print("Accuracy :",acc)

# Plotting the scatter graph
plt.scatter(X_Test[:, 0], X_Test[:, 1], c=Y_Test, cmap='viridis')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
`;

        var text2 = `# Import necessary libraries
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()

# Split the dataset into features and target
X, Y = iris.data, iris.target

# Initialize KMeans clustering with 3 clusters
Km = KMeans(n_clusters=3, random_state=40)

# Fit KMeans to the data
Km.fit(X)

# Get cluster labels
cluster_labels = Km.labels_

# Plot the clusters
plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')
plt.title("Scatter Plot of Iris Dataset using K-Means Clustering\n")
plt.show()
`;

        var text3 = `

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris
print("KFMSCIT007 - Dhanraj Chinta")

# Load the Iris dataset
iris = load_iris()
# Extract features (X) and target (y) from the dataset
X = iris.data
y = iris.target

# Split the data into training and testing sets
# X_train, X_test: input data for training and testing
# y_train, y_test: corresponding target labels for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize Gaussian Naive Bayes model
GNB = GaussianNB()

# Train the model using the training data
GNB.fit(X_train, y_train)

# Predict using the trained model on the test data
y_pred = GNB.predict(X_test)

# Predict the species with user input
user_input = [[5.9, 3.0, 5.1, 1.8]]  # Example user input
predicted_index = GNB.predict(user_input)
predicted_species = iris.target_names[predicted_index]
print("Predicted species for user input:", predicted_species)

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy Score:", accuracy)
# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred))
`;

        var text4 = `

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(0)
X = np.linspace(0, 5, 100).reshape(-1, 1)
Y = 2 * np.sin(X) + np.random.normal(0, 0.5, size=X.shape)

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Define a function to fit polynomial regression models of varying degrees
def fit_polynomial(X_train, Y_train, X_test, Y_test, degree):
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    model = LinearRegression()
    model.fit(X_train_poly, Y_train)

    train_rmse = np.sqrt(mean_squared_error(Y_train, model.predict(X_train_poly)))
    test_rmse = np.sqrt(mean_squared_error(Y_test, model.predict(X_test_poly)))

    return model, train_rmse, test_rmse

# Fit polynomial regression models of varying degrees
degrees = [1, 3, 10]
models = []
train_rmse_values = []
test_rmse_values = []

for degree in degrees:
    model, train_rmse, test_rmse = fit_polynomial(X_train, Y_train, X_test, Y_test, degree)
    models.append(model)
    train_rmse_values.append(train_rmse)
    test_rmse_values.append(test_rmse)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X_train, Y_train, color='blue', label='Training Data')
plt.scatter(X_test, Y_test, color='green', label='Testing Data')x_values = np.linspace(0, 5, 100).reshape(-1, 1)
for i, model in enumerate(models):
    y_values = model.predict(PolynomialFeatures(degree=degrees[i]).fit_transform(x_values))
    plt.plot(x_values, y_values, label=f'Degree {degrees[i]}')

plt.title('Polynomial Regression with Different Degrees\n')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()
`;

        var text5 = `

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv("Salary_Data.csv")

# Extract features (Years of Experience) and target (Salary)
x = data['YearsExperience'].values.reshape(-1, 1)
y = data["Salary"].values

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=99)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(x_train, y_train)

# Make predictions on the testing set
predictions = model.predict(x_test)

# Calculate R-squared score
r2 = r2_score(y_test, predictions)
print("R-squared Score:", r2)
print("Dhanraj Chinta - KFMSCIT007")

# Plot the actual vs. predicted values
plt.scatter(x_test, y_test, color='blue', label='Actual')
plt.scatter(x_test, predictions, color='red', label='Predicted')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.title('Linear Regression: Salary Prediction\n')
plt.legend()
plt.show()

`;

        var text6 = `

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.svm import SVC

# Load the iris dataset
data = pd.read_csv('iris.csv')

# Separate features (X) and target (Y)
X = data.iloc[:, :-1].values
Y = data.iloc[:, -1].values

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=10)

# Initialize and train the Support Vector Classifier (SVC)
model = SVC()
model.fit(X_train, Y_train)

# Predict the target labels for the test set
Y_pred = model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred, average='macro')
recall = recall_score(Y_test, Y_pred, average='macro')
f1 = f1_score(Y_test, Y_pred, average='macro')

print("")
print("---------------------------")
# Print the evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

`;

        var text7 = `
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Sample data
data = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.5], [7, 9], [9, 10], [5.5, 8.5]])

# Number of clusters (k)
k = 3

# Applying KMeans clustering
kmeans = KMeans(n_clusters=k, random_state=0)
clusters_labels = kmeans.fit_predict(data)

# Plotting clustered data
plt.scatter(data[:, 0], data[:, 1], c=clusters_labels,label='Data Points')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=140, c='green', label='Cluster Centers')
plt.xlabel('X Coordinate')
plt.ylabel('Y Coordinate')
plt.title('\nKMeans Clustering (k=' + str(k) + ')')
plt.legend()
plt.show()


`;

        var text8 = `

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Sample data representing (Income and Spending)
data = pd.DataFrame({'Income': [25000, 50000, 75000, 100000, 125000, 150000, 175000, 200000],
                     'Spending': [15000, 25000, 35000, 45000, 55000, 65000, 75000, 85000]})

# Number of clusters (k)
k = 3

# Applying KMeans clustering
kmeans = KMeans(n_clusters=k, random_state=0)
clusters_labels = kmeans.fit_predict(data)

# Plotting clustered data
plt.scatter(data['Income'], data['Spending'], c=clusters_labels, cmap='viridis', label='Data Points')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=140, c='green', label='Cluster Centers')
plt.xlabel('Income')
plt.ylabel('Spending')
plt.title('Dhanraj Chinta - KFMSCIT007\nKMeans Clustering (k=' + str(k) + ')')
plt.legend()
plt.show()

`;

    var text9 = `

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load the Iris dataset
data = pd.read_csv('iris.csv')
# Separate features (X) and target (y)
X = data.drop('species', axis=1)
y = data['species']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Decision Tree Classifier
model = DecisionTreeClassifier()
model.fit(X_train, y_train)
# Make predictions on the testing set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print classification report for more evaluation metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Visualize the decision tree
plt.figure(figsize=(12, 8))
plot_tree(model, filled=True, feature_names=X.columns, class_names=y.unique().astype(str))
plt.title("Decision Tree Visualization \n")
plt.show()
    `;

        function copyText(text) {

            navigator.clipboard.writeText(text).then(function () {

                document.getElementById('copiedMsg').innerHTML = "Text Copied"

            }).catch(function (err) {

                console.error('Unable to copy text', err);

            });

        }

    </script>



</body>



</html>
